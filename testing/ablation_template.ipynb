{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef115b83",
   "metadata": {},
   "source": [
    "# Ablation study. Classic approach\n",
    "\n",
    "Ablation for approaches based on [Beam Search with CaleyPy](https://www.kaggle.com/code/fedimser/beam-search-with-cayleypy) notebook by [Dmytro Fedoriaka](https://www.kaggle.com/fedimser) and [Chervov, A. et al. \"A Machine Learning Approach That Beats Large Rubik’s Cubes: The CayleyPy Project\". arXiv:2502.13266 [cs.LG]. 2025.](https://arxiv.org/abs/2502.13266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import shap\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from cayleypy import PermutationGroups, CayleyGraph, Predictor, prepare_graph\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('../data')\n",
    "assert DATA_ROOT.exists(), f'Dataset directory not found: {DATA_ROOT!s}'\n",
    "\n",
    "TEST_PATH = DATA_ROOT / 'test.csv'\n",
    "SUBMISSION_PATH = Path('submission.csv')  # Kaggle expects this in the working directory\n",
    "TEST_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87681144",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pancake_sort_path(perm: list[int]) -> list[str]:\n",
    "    \"\"\"Return a sequence of prefix reversals that sorts `perm` to the identity permutation.\"\"\"\n",
    "    arr = list(perm)\n",
    "    n = len(arr)\n",
    "    moves: list[str] = []\n",
    "\n",
    "    for target in range(n, 1, -1):\n",
    "        desired_value = target - 1\n",
    "        idx = arr.index(desired_value)\n",
    "\n",
    "        if idx == target - 1:\n",
    "            continue  # already in place\n",
    "\n",
    "        if idx != 0:\n",
    "            moves.append(f'R{idx + 1}')\n",
    "            arr[: idx + 1] = reversed(arr[: idx + 1])\n",
    "\n",
    "        moves.append(f'R{target}')\n",
    "        arr[:target] = reversed(arr[:target])\n",
    "\n",
    "    return moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc67d14",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class Net(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4da8c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, X, y, learning_rate=0.001):\n",
    "    val_ratio = 0.1\n",
    "    batch_size = 512 #1024\n",
    "    dataset = TensorDataset(X, y.float())\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    train_size = len(dataset)-val_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    \n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            total_val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / train_size\n",
    "    avg_val_loss = total_val_loss / val_size\n",
    "\n",
    "    return avg_train_loss, avg_val_loss\n",
    "\n",
    "def train_n(n, width=1000, length=20, n_dim=128, epochs=30):\n",
    "    print(f\"train for {n} permautations\")\n",
    "\n",
    "    graph=CayleyGraph(PermutationGroups.pancake(n), device=\"cuda\", random_seed=42)\n",
    "    X, y = graph.random_walks(width=width, length=min(length, 2*n), mode=\"bfs\")\n",
    "\n",
    "    input_size = graph.definition.state_size\n",
    "    num_classes = n\n",
    "\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = Net(input_size, num_classes, [n_dim]).to(graph.device)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        avg_train_loss, avg_val_loss = train_one_epoch(model, X, y)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {i} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(x=range(epochs), y=train_losses, label='Train Loss', marker='o')\n",
    "    sns.lineplot(x=range(epochs), y=val_losses, label='Val Loss', marker='o')\n",
    "    plt.title(f'Loss for n={n}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model = model.cpu()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return model, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7dbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prefix_length(generator_perm: list[int]) -> int:\n",
    "    n = len(generator_perm)\n",
    "    for i in range(1, n):\n",
    "        if generator_perm[i] != generator_perm[i-1] - 1:\n",
    "            return i \n",
    "    \n",
    "    return n \n",
    "    \n",
    "def convert_to_rk_format(internal_path: list[int], graph: CayleyGraph) -> list[str]:\n",
    "    moves = []\n",
    "    generators = graph.definition.generators\n",
    "    \n",
    "    for move_index in internal_path:\n",
    "        if 0 <= move_index < len(generators):\n",
    "            generator_perm = generators[move_index]\n",
    "\n",
    "            k = find_prefix_length(generator_perm)\n",
    "            moves.append(f'R{k}')\n",
    "    \n",
    "    return moves\n",
    "   \n",
    "def solve(permutation: list[int], graph: CayleyGraph, model: torch.nn.Module, beam_width: int=10000) -> str:\n",
    "    start_state = np.array(permutation)\n",
    "    n = len(permutation) \n",
    "    result = graph.beam_search(\n",
    "        start_state=start_state,\n",
    "        beam_width=beam_width,\n",
    "        max_steps=3*n,\n",
    "        predictor=Predictor(graph, model),\n",
    "        return_path=True\n",
    "    )\n",
    "    \n",
    "    if result.path_found:\n",
    "        moves = convert_to_rk_format(result.path, graph)\n",
    "        return '.'.join(moves)\n",
    "    else:\n",
    "        return 'UNSOLVED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca93ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_success(predictor, beam_width, test_start_states, heurestic_path, graph, max_steps_multiplier=2):\n",
    "    success_count = 0\n",
    "    sum_length = 0\n",
    "    diff_length = 0\n",
    "    optimal_count = 0\n",
    "    \n",
    "    for state, hp in tqdm(zip(test_start_states, heurestic_path), total=len(test_start_states), desc=f\"Beam {beam_width}\"):\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                result = graph.beam_search(\n",
    "                    start_state=state, \n",
    "                    beam_width=beam_width, \n",
    "                    max_steps=len(state)*max_steps_multiplier, \n",
    "                    predictor=predictor,\n",
    "                    return_path=True\n",
    "                )\n",
    "            \n",
    "            if result.path_found:\n",
    "                success_count += 1\n",
    "                path_length = len(result.path)\n",
    "                sum_length += path_length\n",
    "                diff_length += path_length - len(hp)\n",
    "                \n",
    "                if path_length == len(hp):\n",
    "                    optimal_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error in beam search: {e}\")\n",
    "            continue\n",
    "    \n",
    "    success_rate = success_count / len(test_start_states) if test_start_states else 0\n",
    "    avg_length = sum_length / success_count if success_count > 0 else 0\n",
    "    avg_diff = diff_length / success_count if success_count > 0 else 0\n",
    "    optimality_rate = optimal_count / success_count if success_count > 0 else 0\n",
    "    \n",
    "    return success_rate, avg_length, avg_diff, optimality_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def statistical_analysis(df):\n",
    "    print(\"\\n=== STATISTICAL ANALYSIS ===\")\n",
    "    \n",
    "    numeric_cols = ['success_rate', 'optimality_rate', 'avg_solution_length', \n",
    "                   'width', 'length', 'n_dim', 'epochs', 'beam_width', 'n']\n",
    "    numeric_df = df[numeric_cols].select_dtypes(include=[np.number])\n",
    "    \n",
    "    if not numeric_df.empty:\n",
    "        correlation_matrix = numeric_df.cr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.3f')\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    try:\n",
    "        shap.initjs()\n",
    "        \n",
    "        X = df[['width', 'length', 'n_dim', 'epochs', 'beam_width', 'n']]\n",
    "        y_success = df['success_rate']\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y_success)\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X, show=False)\n",
    "        plt.title('SHAP Summary Plot - Success Rate')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_intermediate_results(results, experiment_id):\n",
    "    df = pd.DataFrame([{\n",
    "        **{'experiment_id': r['experiment_id'], 'n': r['n']},\n",
    "        **r['hyperparameters'],\n",
    "        **r['metrics']\n",
    "    } for r in results])\n",
    "    \n",
    "    df.to_csv(f'intermediate_results_{experiment_id}.csv', index=False)\n",
    "\n",
    "def save_final_results(results):\n",
    "    df = pd.DataFrame([{\n",
    "        **{'experiment_id': r['experiment_id'], 'n': r['n']},\n",
    "        **r['hyperparameters'],\n",
    "        **r['metrics']\n",
    "    } for r in results])\n",
    "    \n",
    "    df.to_csv('final_ablation_results.csv', index=False)\n",
    "    \n",
    "    with open('final_ablation_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9877836",
   "metadata": {},
   "source": [
    "## Ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comprehensive_analysis(df):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Success rate vs n\n",
    "    for bw in df['beam_width'].unique():\n",
    "        subset = df[df['beam_width'] == bw]\n",
    "        success_by_n = subset.groupby('n')['success_rate'].mean()\n",
    "        axes[0,0].plot(success_by_n.index, success_by_n.values, 'o-', label=f'beam={bw}')\n",
    "    axes[0,0].set_xlabel('n')\n",
    "    axes[0,0].set_ylabel('Success Rate')\n",
    "    axes[0,0].set_title('Success Rate vs n')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # 2. Effect of  width on success rate\n",
    "    for n in df['n'].unique()[:3]:\n",
    "        subset = df[df['n'] == n]\n",
    "        axes[0,1].scatter(subset['width'], subset['success_rate'], label=f'n={n}', alpha=0.6)\n",
    "    axes[0,1].set_xlabel('Training Width')\n",
    "    axes[0,1].set_ylabel('Success Rate')\n",
    "    axes[0,1].set_title('Effect of Training Width')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Effect of  n_dim on производительность\n",
    "    n_dim_performance = df.groupby('n_dim')['success_rate'].mean()\n",
    "    axes[0,2].bar(n_dim_performance.index, n_dim_performance.values)\n",
    "    axes[0,2].set_xlabel('Hidden Dimension')\n",
    "    axes[0,2].set_ylabel('Average Success Rate')\n",
    "    axes[0,2].set_title('Effect of Hidden Dimension')\n",
    "    \n",
    "    # 4. Heatmap: beam_width vs n\n",
    "    pivot_success = df.pivot_table(values='success_rate', index='n', columns='beam_width', aggfunc='mean')\n",
    "    sns.heatmap(pivot_success, annot=True, fmt='.2f', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Success Rate Heatmap')\n",
    "    \n",
    "    # 5. Optimality rate analysis\n",
    "    optimal_by_n = df.groupby('n')['optimality_rate'].mean()\n",
    "    axes[1,1].plot(optimal_by_n.index, optimal_by_n.values, 'o-', color='red')\n",
    "    axes[1,1].set_xlabel('n')\n",
    "    axes[1,1].set_ylabel('Optimality Rate')\n",
    "    axes[1,1].set_title('Optimality Rate vs n')\n",
    "    \n",
    "    # 6. Trade-off: success rate vs solution length\n",
    "    axes[1,2].scatter(df['success_rate'], df['avg_solution_length'], c=df['n'], alpha=0.6)\n",
    "    axes[1,2].set_xlabel('Success Rate')\n",
    "    axes[1,2].set_ylabel('Average Solution Length')\n",
    "    axes[1,2].set_title('Success Rate vs Solution Length')\n",
    "    plt.colorbar(axes[1,2].collections[0], ax=axes[1,2], label='n')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_baseline_comparison(baseline_results):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    for i, result in enumerate(baseline_results):\n",
    "        n = result['n']\n",
    "        \n",
    "        # Success rate comparison\n",
    "        axes[0,0].plot(result['beam_size_range'], result['hamming_success'], 'o--', label=f'Hamming n={n}')\n",
    "        axes[0,0].plot(result['beam_size_range'], result['nn_success'], 'o-', label=f'NN n={n}')\n",
    "        \n",
    "        # Optimality rate comparison  \n",
    "        axes[0,1].plot(result['beam_size_range'], result['hamming_optimal'], 'o--', label=f'Hamming n={n}')\n",
    "        axes[0,1].plot(result['beam_size_range'], result['nn_optimal'], 'o-', label=f'NN n={n}')\n",
    "    \n",
    "    axes[0,0].set_xlabel('Beam Size')\n",
    "    axes[0,0].set_ylabel('Success Rate')\n",
    "    axes[0,0].set_title('Success Rate Comparison')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].set_xscale('log')\n",
    "    \n",
    "    axes[0,1].set_xlabel('Beam Size')\n",
    "    axes[0,1].set_ylabel('Optimality Rate')\n",
    "    axes[0,1].set_title('Optimality Rate Comparison')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('baseline_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117571e",
   "metadata": {},
   "source": [
    "## Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692787c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study(n_range: list[int], hyperparameter_space: dict):\n",
    "    seed_everything(42)\n",
    "    results = []\n",
    "\n",
    "    experiment_id = 1\n",
    "    total_experiments = len(n_range) * len(list(product(*hyperparameter_space.values())))\n",
    "    \n",
    "    print(f\"Total experiments: {total_experiments}\")\n",
    "    \n",
    "    for n in n_range:\n",
    "        print(f\"\\n=== Experiment for {n} ===\")\n",
    "        \n",
    "        seed_everything(0)\n",
    "        test_start_states = [np.random.permutation(n) for _ in range(100)]\n",
    "        heurestic_path = [pancake_sort_path(x) for x in test_start_states]\n",
    "        \n",
    "        hyperparam_combinations = list(product(\n",
    "            hyperparameter_space['width'],\n",
    "            hyperparameter_space['length'], \n",
    "            hyperparameter_space['n_dim'],\n",
    "            hyperparameter_space['epochs'],\n",
    "            hyperparameter_space['beam_width']\n",
    "        ))\n",
    "        \n",
    "        for width, length, n_dim, epochs, beam_width in tqdm(hyperparam_combinations, desc=f\"n={n}\"):\n",
    "            try:\n",
    "                model, graph = train_n(n, width, length, n_dim, epochs)\n",
    "                predictor = Predictor(graph, model)\n",
    "                \n",
    "                success_rate, avg_length, avg_diff, optimality_rate = measure_success(\n",
    "                    predictor, beam_width, test_start_states, heurestic_path, graph\n",
    "                )\n",
    "                \n",
    "                result = {\n",
    "                    'experiment_id': experiment_id,\n",
    "                    'n': n,\n",
    "                    'hyperparameters': {\n",
    "                        'width': width,\n",
    "                        'length': length, \n",
    "                        'n_dim': n_dim,\n",
    "                        'epochs': epochs,\n",
    "                        'beam_width': beam_width\n",
    "                    },\n",
    "                    'metrics': {\n",
    "                        'success_rate': success_rate,\n",
    "                        'avg_solution_length': avg_length,\n",
    "                        'avg_diff_from_optimal': avg_diff,\n",
    "                        'optimality_rate': optimality_rate\n",
    "                    },\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                experiment_id += 1\n",
    "                \n",
    "                if experiment_id % 50 == 0:\n",
    "                    save_intermediate_results(results, experiment_id)\n",
    "                    \n",
    "                del model, predictor\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Exception while compute {experiment_id}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    save_final_results(results)\n",
    "    \n",
    "    analyze_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_with_baselines(n_range, beam_size_range):\n",
    "    baseline_results = []\n",
    "    \n",
    "    for n in n_range:\n",
    "        print(f\"\\n=== Compare with baseline for n={n} ===\")\n",
    "        \n",
    "        seed_everything(0)\n",
    "        test_start_states = [np.random.permutation(n) for _ in range(50)]\n",
    "        heurestic_path = [pancake_sort_path(x) for x in test_start_states]\n",
    "\n",
    "        graph = CayleyGraph(PermutationGroups.pancake(n), device=\"cuda\", random_seed=42)\n",
    "        \n",
    "        hamming_success_rates = []\n",
    "        hamming_optimal_rates = []\n",
    "        \n",
    "        best_nn_success_rates = []\n",
    "        best_nn_optimal_rates = []\n",
    "        \n",
    "        for beam_size in beam_size_range:\n",
    "            hamming_predictor = Predictor(graph, \"hamming\")\n",
    "            hamming_success, _, _, hamming_optimal = measure_success(\n",
    "                hamming_predictor, beam_size, test_start_states, heurestic_path, graph\n",
    "            )\n",
    "            hamming_success_rates.append(hamming_success)\n",
    "            hamming_optimal_rates.append(hamming_optimal)\n",
    "\n",
    "            best_width, best_length, best_n_dim, best_epochs = 1000, 20, 128, 30\n",
    "            nn_model, _ = train_n(n, best_width, best_length, best_n_dim, best_epochs)\n",
    "            nn_predictor = Predictor(graph, nn_model)\n",
    "            nn_success, _, _, nn_optimal = measure_success(\n",
    "                nn_predictor, beam_size, test_start_states, heurestic_path, graph\n",
    "            )\n",
    "            best_nn_success_rates.append(nn_success)\n",
    "            best_nn_optimal_rates.append(nn_optimal)\n",
    "            \n",
    "            del nn_model, nn_predictor\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        baseline_results.append({\n",
    "            'n': n,\n",
    "            'beam_size_range': beam_size_range,\n",
    "            'hamming_success': hamming_success_rates,\n",
    "            'hamming_optimal': hamming_optimal_rates,\n",
    "            'nn_success': best_nn_success_rates,\n",
    "            'nn_optimal': best_nn_optimal_rates\n",
    "        })\n",
    "    \n",
    "    return baseline_results\n",
    "\n",
    "\n",
    "def analyze_results(results):\n",
    "    df = pd.DataFrame([{\n",
    "        **{'experiment_id': r['experiment_id'], 'n': r['n']},\n",
    "        **r['hyperparameters'], \n",
    "        **r['metrics']\n",
    "    } for r in results])\n",
    "    \n",
    "\n",
    "    best_by_n = df.loc[df.groupby('n')['success_rate'].idxmax()]\n",
    "    print(\"\\nЛучшие результаты по n:\")\n",
    "    print(best_by_n[['n', 'success_rate', 'optimality_rate', 'width', 'length', 'n_dim', 'epochs', 'beam_width']])\n",
    "    \n",
    "\n",
    "    plot_comprehensive_analysis(df)\n",
    "    \n",
    "\n",
    "    statistical_analysis(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503a4ab",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
